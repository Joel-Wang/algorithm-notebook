- 传统方法：基于全量数据，凸优化；当前如何设计适用于当前大规模的、高度非凸的优化方法，比如Adam
- 问题：
- 有监督和无约束问题的优化方法有哪些？
- 随机梯度下降（SGD）为什么会失效？
- ​
- answer:
- 有监督的损失函数：
  - 0-1：非凸费光滑
  - hinge：在fy=1不可导，使用次梯度下降法求解，对fy>=1样本不做惩罚；
  - logistics：出处光滑，可用GD求解，但是对所有样本都有乘法，因此对异常值敏感；
  - cross entroy：交叉熵，
  - square：可使用GD优化，对异常值敏感；
  - absolute：对异常值更鲁棒，但在f=y处没有办法求导；
  - huber：|f-y|较小时为平方损失，较大时为线性损失，处处可导，对异常点鲁邦；
- 无约束问题的优化方法：
  - min L(θ)
  - 直接法：满足两点条件：1为凸函数，2L(θ)求导为0有闭式解；
  - 迭代法：δ(L(θ))为L对θ一阶导数，δ^2(L(θ))为2阶导数；
    - 一阶法：GD θ=θ-αδ(L(θ))，收敛速度慢，但计算简单；
    - 二阶法：牛顿法 θ=θ-δ(L(θ)) / δ^2(L(θ))，收敛速度快，但计算复杂；
- 随机梯度下降（SGD）：
  - 由于每步接受信息有限，因此目标函数的收敛情况很不稳定，容易受到山谷和鞍点的影响；
  - 改进：
  - 动量方法：根据前一次更新步幅的衰减量结合当前梯度来进行更新
  - 感知方法：更新频率高的参数学习率较小，更新频率低的参数学习率较大，总体的学习率越来越小，保证算法最终收敛；具体是通过历史 梯度平方和来衡量；
  - Adam方法：通过一阶矩m和二阶矩v来将惯性和感知结合起来；
    - 当m较大，v较大：正常更新
    - m接近0，v较大：当前遇到了峡谷，容易引起反弹震荡；
    - 当m较大，v接近0时：不可能出现；
    - m，v都接近0时，可能走到了平原，也可能更新结束；
- L1正则与稀疏性的关系：
  - L1使得模型参数稀疏的原理是什么？
    - 因为加了正则项，相当于加了约束，即范数不能大于一个数，记为m。L2的相当于定义了一个圆形的解空间，L1的解空间是多边形；L1更容易与目标函数在角点碰撞，从而产生稀疏解。
- point:
  - p146主成分分析的优化问题？
  - p153直接使用数值梯度的问题？数值梯度代替解析梯度？
  - p167贝叶斯先验介绍L1稀疏性？

